{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курсовой проект по курсу \"Потоковая обработка данных\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание задачи: Спрогнозировать, будет ли кандидат будет работать в компании-нанимателе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылка на набор даных: https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists\n",
    "\n",
    "**Описание датасета:**\n",
    "Компания, занимающаяся Big Data и Data Science, хочет нанять специалистов по данным из числа людей, успешно прошедших курсы, проводимые компанией. Многие люди записываются на обучение. Компания хочет знать, кто из этих кандидатов действительно хочет работать в компании после обучения или ищет новую работу, потому что это помогает снизить затраты и время, а также качество обучения или планирование курсов и категоризацию кандидатов . Информация, связанная с демографическими данными, образованием, опытом, поступает от кандидатов при регистрации и зачислении.\n",
    "\n",
    "Вся выборка разделена на обучающую и тестовую. Цель (`target`) не включена в тестовый датасет.\n",
    "\n",
    "**NB!** Набор данных несбалансирован. Большинство функций являются категориальными (номинальные, порядковые, двоичные), некоторые имеют высокую мощность.\n",
    "\n",
    "Признаки:\n",
    "- enrollee_id: уникальный идентификатор кандидата\n",
    "- city: Код города\n",
    "- city_ development_index: индекс развития города (в масштабе)\n",
    "- gender: Пол кандидата\n",
    "- relvent_experience: Соответствующий опыт кандидата\n",
    "- enrolled_university: Тип зачисленных университетских курсов, если таковые имеются.\n",
    "- education_level: Уровень образования кандидата\n",
    "- major_discipline: Обучение основной дисциплине кандидата\n",
    "- experience: Кандидатский общий стаж в годах\n",
    "- company_size: Количество сотрудников в компании текущего работодателя\n",
    "- company_type: Тип текущего работодателя\n",
    "- last_new_job: разница в годах между предыдущей работой и текущей работой\n",
    "- training_hours: завершенные часы обучения\n",
    "- tagret: 0 - Не ищу смены работы, 1 - Ищу смену работы\n",
    "\n",
    "Данная задача является задачей бинарной классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Описание работы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель состоит в демонстрации возможностей совместной работы spark, kafka и cassandra на примере задачи по предсаказанию (см.выше).\n",
    "\n",
    "Обучаем на тренировочном набое данных (далее - train) модель и затем делаем предсказания на на потоке данных, которые предварительно аггрегируются в таблицей из cassandra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка файлов перед загрузкой на сервер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:46:15.734178Z",
     "start_time": "2021-04-05T16:46:13.564982Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:50:22.210383Z",
     "start_time": "2021-04-05T16:50:22.114614Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/aug_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:50:24.615924Z",
     "start_time": "2021-04-05T16:50:24.583015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19158 entries, 0 to 19157\n",
      "Data columns (total 14 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   enrollee_id             19158 non-null  int64  \n",
      " 1   city                    19158 non-null  object \n",
      " 2   city_development_index  19158 non-null  float64\n",
      " 3   gender                  14650 non-null  object \n",
      " 4   relevent_experience     19158 non-null  object \n",
      " 5   enrolled_university     18772 non-null  object \n",
      " 6   education_level         18698 non-null  object \n",
      " 7   major_discipline        16345 non-null  object \n",
      " 8   experience              19093 non-null  object \n",
      " 9   company_size            13220 non-null  object \n",
      " 10  company_type            13018 non-null  object \n",
      " 11  last_new_job            18735 non-null  object \n",
      " 12  training_hours          19158 non-null  int64  \n",
      " 13  target                  19158 non-null  float64\n",
      "dtypes: float64(2), int64(2), object(10)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T16:50:36.059609Z",
     "start_time": "2021-04-05T16:50:36.024702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollee_id</th>\n",
       "      <th>city</th>\n",
       "      <th>city_development_index</th>\n",
       "      <th>gender</th>\n",
       "      <th>relevent_experience</th>\n",
       "      <th>enrolled_university</th>\n",
       "      <th>education_level</th>\n",
       "      <th>major_discipline</th>\n",
       "      <th>experience</th>\n",
       "      <th>company_size</th>\n",
       "      <th>company_type</th>\n",
       "      <th>last_new_job</th>\n",
       "      <th>training_hours</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8949</td>\n",
       "      <td>city_103</td>\n",
       "      <td>0.920</td>\n",
       "      <td>Male</td>\n",
       "      <td>Has relevent experience</td>\n",
       "      <td>no_enrollment</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>&gt;20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29725</td>\n",
       "      <td>city_40</td>\n",
       "      <td>0.776</td>\n",
       "      <td>Male</td>\n",
       "      <td>No relevent experience</td>\n",
       "      <td>no_enrollment</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>15</td>\n",
       "      <td>50-99</td>\n",
       "      <td>Pvt Ltd</td>\n",
       "      <td>&gt;4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11561</td>\n",
       "      <td>city_21</td>\n",
       "      <td>0.624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No relevent experience</td>\n",
       "      <td>Full time course</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>STEM</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>never</td>\n",
       "      <td>83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enrollee_id      city  city_development_index gender  \\\n",
       "0         8949  city_103                   0.920   Male   \n",
       "1        29725   city_40                   0.776   Male   \n",
       "2        11561   city_21                   0.624    NaN   \n",
       "\n",
       "       relevent_experience enrolled_university education_level  \\\n",
       "0  Has relevent experience       no_enrollment        Graduate   \n",
       "1   No relevent experience       no_enrollment        Graduate   \n",
       "2   No relevent experience    Full time course        Graduate   \n",
       "\n",
       "  major_discipline experience company_size company_type last_new_job  \\\n",
       "0             STEM        >20          NaN          NaN            1   \n",
       "1             STEM         15        50-99      Pvt Ltd           >4   \n",
       "2             STEM          5          NaN          NaN        never   \n",
       "\n",
       "   training_hours  target  \n",
       "0              36     1.0  \n",
       "1              47     0.0  \n",
       "2              83     0.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:00:22.282350Z",
     "start_time": "2021-04-05T17:00:22.255453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all features</th>\n",
       "      <th>numeric features</th>\n",
       "      <th>categorical features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       all features  numeric features  categorical features\n",
       "count            12                 3                    10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features = data.select_dtypes(include=[np.number]).drop('target',\n",
    "                                                                  axis=1)\n",
    "categorical_features = data.select_dtypes(include=[np.object])\n",
    "\n",
    "pd.DataFrame({'all features': len(data.columns.tolist()[2:]),\n",
    "              'numeric features': numerical_features.shape[1],\n",
    "              'categorical features': categorical_features.shape[1]}, index=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:02:22.444069Z",
     "start_time": "2021-04-05T17:02:22.437086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_features:  ['enrollee_id', 'city_development_index', 'training_hours'] \n",
      "\n",
      "categorical_features:  ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\n"
     ]
    }
   ],
   "source": [
    "print('numerical_features: ', numerical_features.columns.tolist(), '\\n')\n",
    "print('categorical_features: ', categorical_features.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка для загрузки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:05:45.140459Z",
     "start_time": "2021-04-05T17:05:45.013797Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/aug_train.csv')\n",
    "test = pd.read_csv('./data/aug_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T17:09:37.150119Z",
     "start_time": "2021-04-03T17:09:37.109227Z"
    }
   },
   "source": [
    "#### Для кассандры объединяем train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:05:47.911049Z",
     "start_time": "2021-04-05T17:05:47.892101Z"
    }
   },
   "outputs": [],
   "source": [
    "aug_all_for_cassandra = train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для кафки выделяем из test уникальные `enrollee_id` и добавляем к ним синтетический столбец с `datetime`, имитирующий поступление данных по времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:08:39.808566Z",
     "start_time": "2021-04-05T17:08:39.771661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enrollee_id</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32403</td>\n",
       "      <td>2021-01-03 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9858</td>\n",
       "      <td>2021-01-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31806</td>\n",
       "      <td>2021-01-04 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   enrollee_id            datetime\n",
       "0        32403 2021-01-03 23:00:00\n",
       "1         9858 2021-01-04 00:00:00\n",
       "2        31806 2021-01-04 01:00:00"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enrollee_id_for_kafka = pd.DataFrame(test['enrollee_id'])\n",
    "start_date = pd.to_datetime('1/4/2021')\n",
    "enrollee_id_for_kafka['datetime'] = pd \\\n",
    "    .to_timedelta(enrollee_id_for_kafka.index - 1, unit='H') + start_date\n",
    "enrollee_id_for_kafka.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T17:10:12.859168Z",
     "start_time": "2021-04-03T17:10:12.585903Z"
    }
   },
   "source": [
    "#### Сохранение датафреймов в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T17:10:12.859168Z",
     "start_time": "2021-04-03T17:10:12.585903Z"
    }
   },
   "outputs": [],
   "source": [
    "aug_all_for_cassandra.to_csv('./data/aug_all_for_cassandra.csv')\n",
    "enrollee_id_for_kafka.to_csv('./data/enrollee_id_for_kafka.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загружаем необходимые файлы на удаленный сервер с помошью mobaxtern и копируем файл на hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hdfs dfs -put for_stream/aug_train.csv input_csv_for_stream\n",
    "hdfs dfs -put for_stream/aug_all_for_cassandra.csv input_csv_for_stream\n",
    "hdfs dfs -put for_stream/enrollee_id_for_kafka.csv input_csv_for_stream\n",
    "\n",
    "hdfs dfs -ls input_csv_for_stream\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](img/2021-04-05_223454.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных в kafka и cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка в kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создадим новый топик `enrollee_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "/kafka/bin/kafka-topics.sh \\\n",
    "    --create --topic enrollee_id \\\n",
    "    --zookeeper 10.0.0.6:2181 \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 2 \\\n",
    "    --config retention.ms=-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запись в kafka статичного DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:31:04.786892Z",
     "start_time": "2021-04-05T17:31:04.687161Z"
    }
   },
   "source": [
    "```python\n",
    "# %load aug_to_kafka.py\n",
    "\"\"\"\n",
    "export SPARK_KAFKA_VERSION=0.10\n",
    "/spark2.4/bin/pyspark \\\n",
    "    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 \\\n",
    "    --driver-memory 512m \\\n",
    "    --num-executors 1 \\\n",
    "    --executor-memory 512m \\\n",
    "    --master local[1]\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StringType, StructField\n",
    "\n",
    "spark = SparkSession.builder.appName(\"gogin_spark\").getOrCreate()\n",
    "\n",
    "kafka_brokers = \"10.0.0.6:6667\"\n",
    "\n",
    "enrollee_schema = StructType([StructField('c0', StringType()),\n",
    "                              StructField('enrollee_id', StringType()),\n",
    "                              StructField('datetime', StringType())])\n",
    "\n",
    "enrollee_id_df = spark.read \\\n",
    "    .options(delimiter=',', schema=enrollee_schema, header=True) \\\n",
    "    .csv(path=\"input_csv_for_stream/enrollee_id_for_kafka.csv\") \\\n",
    "    .select('enrollee_id', 'datetime')\n",
    "\n",
    "enrollee_id_df \\\n",
    "    .selectExpr(\"CAST(null AS STRING) as key\", \"CAST(to_json(struct(*)) AS STRING) as value\") \\\n",
    "    .write \\\n",
    "    .format(source=\"kafka\") \\\n",
    "    .option(\"topic\", \"enrollee_id\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/enrollee_id_for_kafka\") \\\n",
    "    .save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка записанныз данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:31:04.786892Z",
     "start_time": "2021-04-05T17:31:04.687161Z"
    }
   },
   "source": [
    "```python\n",
    "def console_output(df, freq, truncate=True, numRows=20):\n",
    "    \"\"\"\n",
    "    Вывод на консоль вместо show()\n",
    "    :param numRows: number of lines to display\n",
    "    :param df: spark DataFrame\n",
    "    :param freq: frequency in seconds\n",
    "    :param truncate: truncate values, bool\n",
    "    \"\"\"\n",
    "    return df.writeStream \\\n",
    "        .format(source=\"console\") \\\n",
    "        .trigger(processingTime='%s seconds' % freq) \\\n",
    "        .options(truncate=truncate, numRows=numRows) \\\n",
    "        .start()\n",
    "\n",
    "\n",
    "# Проверка, прочитали потихоньку\n",
    "raw_data = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", \"enrollee_id\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1\") \\\n",
    "    .load() \\\n",
    "    .select(F.col(\"value\").cast(\"String\"), \"offset\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), enrollee_schema).alias(\"value\"), \"offset\") \\\n",
    "    .select(\"value.*\", \"offset\") \\\n",
    "    .select('enrollee_id', 'datetime', 'offset')\n",
    "\n",
    "out = console_output(df=raw_data, freq=5, truncate=False)\n",
    "out.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:53:16.393056Z",
     "start_time": "2021-04-05T18:53:16.385077Z"
    }
   },
   "source": [
    "<img src=\"img/2021-04-05_224640.jpg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка в cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создадим таблицу в кассандре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "/cassandra/bin/cqlsh  # (подключиться к касандре через консоль)\n",
    "\n",
    "# создать схему\n",
    "CREATE  KEYSPACE IF NOT EXISTS koryagin \n",
    "WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n",
    "\n",
    "USE koryagin;\n",
    "\n",
    "# создание таблицы\n",
    "CREATE TABLE IF NOT EXISTS aug_all (\n",
    "    enrollee_id int,\n",
    "    city text,\n",
    "    city_development_index double,\n",
    "    gender text,\n",
    "    relevent_experience text,\n",
    "    enrolled_university text,\n",
    "    education_level text,\n",
    "    major_discipline text,\n",
    "    experience text,\n",
    "    company_size text,\n",
    "    company_type text,\n",
    "    last_new_job text,\n",
    "    training_hours int,\n",
    "    target double,\n",
    "    primary key (enrollee_id));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запись dataframe в cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:54:26.096056Z",
     "start_time": "2021-04-05T17:54:23.577765Z"
    }
   },
   "source": [
    "```python\n",
    "# %load aug_to_cassandra.py\n",
    "\"\"\"\n",
    "export SPARK_KAFKA_VERSION=0.10\n",
    "/spark2.4/bin/pyspark \\\n",
    "    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.datastax.spark:spark-cassandra-connector_2.11:2.4.2 \\\n",
    "    --driver-memory 512m \\\n",
    "    --driver-cores 1 \\\n",
    "    --master local[1]\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"gogin_spark\").getOrCreate()\n",
    "\n",
    "# для начала готовим DataFrame\n",
    "aug_all_df = spark.read \\\n",
    "    .options(delimiter=',', inferschema=True, header=True) \\\n",
    "    .csv(path=\"input_csv_for_stream/aug_all_for_cassandra.csv\") \\\n",
    "    .select('enrollee_id', 'city', 'city_development_index', 'gender', 'relevent_experience',\n",
    "            'enrolled_university', 'education_level', 'major_discipline', 'experience', 'company_size',\n",
    "            'company_type', 'last_new_job', 'training_hours', 'target')\n",
    "\n",
    "# пишем\n",
    "aug_all_df.write \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"aug_all\", keyspace=\"koryagin\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:54:26.096056Z",
     "start_time": "2021-04-05T17:54:23.577765Z"
    }
   },
   "source": [
    "```python\n",
    "# Проверяем - читаем большой большой датасет по ключу\n",
    "cass_big_df = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"aug_all\", keyspace=\"koryagin\") \\\n",
    "    .load()\n",
    "\n",
    "cass_big_df.filter(F.col(\"enrollee_id\") == \"27107\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:54:26.096056Z",
     "start_time": "2021-04-05T17:54:23.577765Z"
    }
   },
   "source": [
    "![Title](img/2021-04-05_225607.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для обучения модели запустить следующий код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T17:59:26.918165Z",
     "start_time": "2021-04-05T17:59:26.906197Z"
    }
   },
   "source": [
    "```python\n",
    "# %load aug_ml_train.py\n",
    "\"\"\"\n",
    "export SPARK_KAFKA_VERSION=0.10\n",
    "/spark2.4/bin/pyspark \\\n",
    "    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.datastax.spark:spark-cassandra-connector_2.11:2.4.2 \\\n",
    "    --driver-memory 512m \\\n",
    "    --driver-cores 1 \\\n",
    "    --master local[1]\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, StructField, FloatType, DoubleType\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, VectorAssembler, StringIndexer, IndexToString\n",
    "\n",
    "spark = SparkSession.builder.appName(\"gogin_spark\").getOrCreate()\n",
    "\n",
    "enrollee_known = spark.read \\\n",
    "    .options(delimiter=',', inferschema=True, header=True) \\\n",
    "    .csv(path=\"input_csv_for_stream/aug_train.csv\")\n",
    "\n",
    "# в общем - все анализируемые колонки заносим в колонку-вектор features\n",
    "categoricalColumns = ['city', 'gender', 'relevent_experience', 'enrolled_university', 'education_level',\n",
    "                      'major_discipline', 'experience', 'company_type', 'last_new_job']\n",
    "numericCols = ['city_development_index', 'training_hours']\n",
    "\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + 'Index').setHandleInvalid(\"keep\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], \n",
    "                                     outputCols=[categoricalCol + \"classVec\"]).setHandleInvalid(\"keep\")\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol='target', outputCol='label').setHandleInvalid(\"keep\")\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\").setHandleInvalid(\"keep\")\n",
    "stages += [assembler]\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "stages += [lr]\n",
    "\n",
    "label_stringIdx_fit = label_stringIdx.fit(dataset=enrollee_known)\n",
    "indexToStringEstimator = IndexToString() \\\n",
    "    .setInputCol(\"prediction\") \\\n",
    "    .setOutputCol(\"prediction_target\") \\\n",
    "    .setLabels(label_stringIdx_fit.labels)\n",
    "\n",
    "stages += [indexToStringEstimator]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = pipeline.fit(dataset=enrollee_known)\n",
    "\n",
    "# сохраняем модель на HDFS\n",
    "pipelineModel.write().overwrite().save(\"ml_models/my_LR_model_enrollees\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для наглядности можно посчитать процент полной сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pipelineModel.transform(enrollee_known).select(\"prediction_target\", \"target\").show(n=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Title](img/2021-04-05_230936.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание на стриме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Написание функций и dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:40:06.686891Z",
     "start_time": "2021-04-05T18:40:06.587157Z"
    }
   },
   "source": [
    "```python\n",
    "# %load aug_ml_predict.py\n",
    "\"\"\"\n",
    "export SPARK_KAFKA_VERSION=0.10\n",
    "/spark2.4/bin/pyspark \\\n",
    "    --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.datastax.spark:spark-cassandra-connector_2.11:2.4.2 \\\n",
    "    --driver-memory 512m \\\n",
    "    --driver-cores 1 \\\n",
    "    --master local[1]\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StringType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"gogin_spark\").getOrCreate()\n",
    "\n",
    "kafka_brokers = \"10.0.0.6:6667\"\n",
    "\n",
    "# читаем кафку по одной записи, но можем и по 1000 за раз\n",
    "enrollee_schema = StructType([StructField('c0', StringType()),\n",
    "                              StructField('enrollee_id', StringType()),\n",
    "                              StructField('datetime', StringType())])\n",
    "\n",
    "enrollees_df = spark.readStream \\\n",
    "    .format(source=\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", \"enrollee_id\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"1\") \\\n",
    "    .load() \\\n",
    "    .select(F.col(\"value\").cast(\"String\"), \"offset\") \\\n",
    "    .select(F.from_json(F.col(\"value\"), enrollee_schema).alias(\"value\"), \"offset\") \\\n",
    "    .select(\"value.*\", \"offset\") \\\n",
    "    .select('enrollee_id', 'datetime', 'offset')\n",
    "\n",
    "enrollees_df.printSchema()\n",
    "\n",
    "\n",
    "def console_output(df, freq, truncate=True, numRows=20):\n",
    "    \"\"\"\n",
    "    Вывод на консоль вместо show()\n",
    "    :param df: spark DataFrame\n",
    "    :param freq: frequency in seconds\n",
    "    :param truncate: truncate values, bool\n",
    "    \"\"\"\n",
    "    return df.writeStream \\\n",
    "        .format(\"console\") \\\n",
    "        .trigger(processingTime='%s seconds' % freq) \\\n",
    "        .options(truncate=truncate) \\\n",
    "        .options(numRows=numRows) \\\n",
    "        .start()\n",
    "\n",
    "\n",
    "# out = console_output(df=enrollees_df, freq=5, truncate=False)\n",
    "# out.stop()\n",
    "\n",
    "###############\n",
    "# подготавливаем DataFrame для запросов к касандре с историческими данными\n",
    "cassandra_features_raw = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"aug_all\", keyspace=\"koryagin\") \\\n",
    "    .load()\n",
    "\n",
    "cassandra_features_selected = cassandra_features_raw.drop(\"target\")\n",
    "# cassandra_features_selected.show(n=5)\n",
    "\n",
    "# подгружаем ML из HDFS\n",
    "pipeline_model = PipelineModel.load(path=\"ml_models/my_LR_model_enrollees\")\n",
    "\n",
    "\n",
    "##########\n",
    "# вся логика в этом foreachBatch\n",
    "def writer_logic(df, epoch_id):\n",
    "    \"\"\" Function for foreachBatch\n",
    "    :param df: stream DataFrame from kafka\n",
    "    :param epoch_id: --\n",
    "    \"\"\"\n",
    "    df.persist()\n",
    "    print(\"---------I've got new batch--------\")\n",
    "    print(\"This is what I've got from Kafka:\")\n",
    "    df.show()\n",
    "    features_from_kafka = df.select(\"enrollee_id\")\n",
    "    #\n",
    "    # оставим только уникальные \"enrollee_id\"\n",
    "    users_list_df = features_from_kafka.distinct()\n",
    "    # превращаем DataFrame(Row) в Array(Row)\n",
    "    users_list_rows = users_list_df.collect()\n",
    "    # превращаем Array(Row) в Array(String)\n",
    "    users_list = map(lambda x: str(x.__getattr__(\"enrollee_id\")), users_list_rows)\n",
    "    # print(users_list)\n",
    "    where_string = \" enrollee_id = \" + \" or enrollee_id = \".join(users_list)\n",
    "    print(\"I'm gonna select this from Cassandra:\")\n",
    "    print(where_string)\n",
    "    features_from_cassandra = cassandra_features_selected \\\n",
    "        .where(where_string) \\\n",
    "        .na.fill(0) \\\n",
    "        .drop(\"enrollee_id\")\n",
    "    features_from_cassandra.persist()\n",
    "    # объединяем микробатч из кафки и микробатч касандры\n",
    "    cassandra_kafka_aggregation = features_from_cassandra\n",
    "    predict = pipeline_model.transform(cassandra_kafka_aggregation)\n",
    "    # predict_short = predict.select('city', 'city_development_index', 'gender', 'relevent_experience',\n",
    "    #                                'enrolled_university', 'education_level', 'major_discipline', 'experience',\n",
    "    #                                'comp,any_size', 'company_type', 'last_new_job', 'training_hours', 'prediction',\n",
    "    #                                F.col(\"prediction_target\").alias(\"target\"))\n",
    "    predict_short_short = features_from_kafka \\\n",
    "        .crossJoin(predict.select('prediction_target'))\n",
    "    print(\"Here is what I've got after model transformation:\")\n",
    "    predict_short_short.show()\n",
    "    # TODO: доделать сохранение предсказания обратно в Cassandra\n",
    "    # обновляем исторический агрегат в касандре\n",
    "    # predict_short.write \\\n",
    "    #     .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    #     .options(table=\"aug_all\", keyspace=\"koryagin\") \\\n",
    "    #     .mode(\"append\") \\\n",
    "    #     .save()\n",
    "    # print(\"I saved the prediction and aggregation in Cassandra. Continue...\")\n",
    "    features_from_cassandra.unpersist()\n",
    "    df.unpersist()\n",
    "\n",
    "\n",
    "# связываем источник Кафки и foreachBatch функцию\n",
    "stream = enrollees_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .foreachBatch(func=writer_logic) \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints/sales_unknown_checkpoint\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:26:37.874611Z",
     "start_time": "2021-04-05T18:26:37.728990Z"
    }
   },
   "source": [
    "#### Поехали"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:28:47.358011Z",
     "start_time": "2021-04-05T18:28:47.314096Z"
    }
   },
   "source": [
    "```python\n",
    "# поехали\n",
    "s = stream.start()\n",
    "\n",
    "s.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:28:47.358011Z",
     "start_time": "2021-04-05T18:28:47.314096Z"
    }
   },
   "source": [
    "![Title](img/2021-04-05_233452.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-05T18:41:04.114896Z",
     "start_time": "2021-04-05T18:41:04.096946Z"
    }
   },
   "outputs": [],
   "source": [
    "# -- The End --"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
